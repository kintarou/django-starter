{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chainer.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/kintarou/django-starter/blob/master/chainer.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Bg2tBipWvAVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a7b581a4-ee93-42d8-c010-e0a40a6c6d4e"
      },
      "cell_type": "code",
      "source": [
        "!pip install chainer\n",
        "!pip install nn\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chainer in /usr/local/lib/python3.6/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (1.14.3)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (3.5.2.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer) (3.0.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->chainer) (39.2.0)\n",
            "Collecting nn\n",
            "  Downloading https://files.pythonhosted.org/packages/32/5b/4ff7a1bdc8e30e260c4dd44343275c50497b9db9fd96100fef1b7ca8a2b6/nn-0.0.9.tar.gz\n",
            "Building wheels for collected packages: nn\n",
            "  Running setup.py bdist_wheel for nn ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/61/36/72/2ccb1d96aae93cf0e1de5b84c0c953de38dc5a152011c4a7f6\n",
            "Successfully built nn\n",
            "Installing collected packages: nn\n",
            "Successfully installed nn-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PyQlSPiu-Vq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from chainer import Chain\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "\n",
        "# ネットワーク定義\n",
        "k = 16\n",
        "fcl = 256\n",
        "class NN(Chain):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv1 = L.Convolution2D(in_channels = 1, out_channels = k, ksize = 3, pad = 1)\n",
        "            self.conv2 = L.Convolution2D(in_channels = k, out_channels = k, ksize = 3, pad = 1)\n",
        "            self.l3    = L.Linear(7*7*k, fcl)\n",
        "            self.l4    = L.Linear(fcl, 10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = self.conv1(F.reshape(x, (len(x), 1, 28, 28)))\n",
        "        h = F.max_pooling_2d(F.relu(h), 2)\n",
        "        h = self.conv2(h)\n",
        "        h = F.max_pooling_2d(F.relu(h), 2)\n",
        "        h = F.relu(self.l3(h))\n",
        "        return self.l4(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDvKQLIJ-YNF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import chainer\n",
        "import chainer.functions as F\n",
        "from chainer import cuda\n",
        "from chainer import datasets, iterators, optimizers, serializers\n",
        "\n",
        "import argparse\n",
        "\n",
        "from nn import NN\n",
        "\n",
        "# 引数の定義\n",
        "parser = argparse.ArgumentParser(description='example: MNIST')\n",
        "parser.add_argument('--batchsize', '-b', type=int, default=100,\n",
        "                    help='Number of images in each mini-batch')\n",
        "parser.add_argument('--epoch', '-e', type=int, default=20,\n",
        "                    help='Number of sweeps over the dataset to train')\n",
        "parser.add_argument('--gpu', '-g', type=int, default=-1,\n",
        "                    help='GPU ID (negative value indicates CPU)')\n",
        "parser.add_argument('--initmodel', '-m', default='',\n",
        "                    help='Initialize the model from given file')\n",
        "parser.add_argument('--resume', '-r', default='',\n",
        "                    help='Resume the optimization from snapshot')\n",
        "args = parser.parse_args()\n",
        "\n",
        "print('GPU: {}'.format(args.gpu))\n",
        "print('# Minibatch-size: {}'.format(args.batchsize))\n",
        "print('# epoch: {}'.format(args.epoch))\n",
        "\n",
        "# モデルの作成\n",
        "model = NN()\n",
        "# モデルをGPUに転送\n",
        "if args.gpu >= 0:\n",
        "    cuda.get_device_from_id(args.gpu).use()\n",
        "    model.to_gpu()\n",
        "\n",
        "# 最適化手法の設定\n",
        "optimizer = optimizers.SGD()\n",
        "optimizer.setup(model)\n",
        "\n",
        "# 保存したモデルを読み込み\n",
        "if args.initmodel:\n",
        "    print('Load model from', args.initmodel)\n",
        "    serializers.load_npz(args.initmodel, model)\n",
        "# 保存した最適化状態を復元\n",
        "if args.resume:\n",
        "    print('Load optimizer state from', args.resume)\n",
        "    serializers.load_npz(args.resume, optimizer)\n",
        "\n",
        "# MNISTデータセットを読み込み\n",
        "train, test = datasets.get_mnist()\n",
        "\n",
        "train_iter = iterators.SerialIterator(train, args.batchsize)\n",
        "test_iter = iterators.SerialIterator(test, args.batchsize, shuffle=False)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(1, args.epoch + 1):\n",
        "    # ミニバッチ単位で学習\n",
        "    sum_loss = 0\n",
        "    itr = 0\n",
        "    for i in range(0, len(train), args.batchsize):\n",
        "        # ミニバッチデータ\n",
        "        train_batch = train_iter.next()\n",
        "        x, t = chainer.dataset.concat_examples(train_batch, args.gpu)\n",
        "\n",
        "        # 順伝播\n",
        "        y = model(x)\n",
        "\n",
        "        # 勾配を初期化\n",
        "        model.cleargrads()\n",
        "        # 損失計算\n",
        "        loss = F.softmax_cross_entropy(y, t)\n",
        "        # 誤差逆伝播\n",
        "        loss.backward()\n",
        "        optimizer.update()\n",
        "\n",
        "        sum_loss += loss.data\n",
        "        itr += 1\n",
        "\n",
        "    # 評価\n",
        "    sum_test_loss = 0\n",
        "    sum_test_accuracy = 0\n",
        "    test_itr = 0\n",
        "    for i in range(0, len(test), args.batchsize):\n",
        "        # ミニバッチデータ\n",
        "        test_batch = test_iter.next()\n",
        "        with chainer.no_backprop_mode():\n",
        "            with chainer.using_config('train', False):\n",
        "                x_test, t_test = chainer.dataset.concat_examples(test_batch, args.gpu)\n",
        "\n",
        "                # 順伝播\n",
        "                y_test = model(x_test)\n",
        "                # 損失計算\n",
        "                sum_test_loss += F.softmax_cross_entropy(y_test, t_test).data\n",
        "                # 一致率計算\n",
        "                sum_test_accuracy += F.accuracy(y_test, t_test).data\n",
        "                test_itr += 1\n",
        "\n",
        "    print('epoch={}, train loss={}, test loss={}, accuracy={}'.format(\n",
        "        optimizer.epoch + 1, sum_loss / itr,\n",
        "        sum_test_loss / test_itr, sum_test_accuracy / test_itr))\n",
        "\n",
        "    optimizer.new_epoch()\n",
        "\n",
        "# モデル保存\n",
        "print('save the model')\n",
        "serializers.save_npz('model', model)\n",
        "# 最適化状態保存\n",
        "print('save the optimizer')\n",
        "serializers.save_npz('state', optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XfVeSC-0-gHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}